# Agent 2.5 LLM Scorer - Quick Start Guide

## Overview

**Agent 2.5 LLM Scorer** uses GPT-OSS-20B via OpenRouter to provide intelligent resume-job matching with natural language reasoning.

### ‚úÖ Advantages over ML Approach

| Feature | ML Scorer | LLM Scorer |
|---------|-----------|------------|
| **Accuracy** | 60% (with limited training) | 85-95% (expected) |
| **Training Required** | Yes (35,730 samples, 15-25 min) | No |
| **Reasoning** | No | Yes (explains matches) |
| **Cost** | Free | ~$0.0008 per match |
| **Speed** | ~0.05s | ~2-5s |
| **Offline** | Yes | No (requires API) |

---

## Installation

### 1. Install Required Package

```powershell
cd C:\Users\DELL\Desktop\HR-Project
pip install openai==1.51.0
```

### 2. Verify API Key

Your OpenRouter API key is already configured:
```
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
```

---

## Usage

### Basic Scoring

```python
from src.agents.agent2_5_llm_scorer import LLMScorer

# Initialize
API_KEY = "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"
scorer = LLMScorer(api_key=API_KEY)

# Prepare data
profile = {
    "name": "John Doe",
    "skills": ["Python", "Django", "PostgreSQL"],
    "years_of_experience": 5,
    "seniority_level": "Mid-level"
}

job = {
    "title": "Senior Backend Developer",
    "skills_required": ["Python", "Django", "Redis", "Docker"],
    "min_experience_years": 4,
    "max_experience_years": 8,
    "seniority_level": "Senior"
}

# Score match
result = scorer.score_match(profile, job)

print(f"Match: {result['match_label']}")
print(f"Confidence: {result['confidence']:.1%}")
print(f"Reasoning: {result['reasoning']}")
```

### Batch Scoring (Multiple Jobs)

```python
# Score 1 profile against multiple jobs
jobs_list = [job1, job2, job3, ...]  # List of job dicts

top_matches = scorer.batch_score(profile, jobs_list, top_k=10)

for i, match in enumerate(top_matches, 1):
    print(f"{i}. {match['job_data']['title']} - {match['match_label']}")
```

---

## Testing

### Run Test Suite

```powershell
cd C:\Users\DELL\Desktop\HR-Project
python tests/test_agent2_5_llm_scorer.py
```

**Tests included:**
1. ‚úÖ Basic scoring (3 test cases: High/Medium/Low matches)
2. ‚úÖ Batch scoring (1 profile vs 4 jobs)
3. ‚úÖ Real data integration (uses parsed profiles from Agent 1)

### Compare ML vs LLM

```powershell
python compare_scorers.py
```

This shows side-by-side comparison of both approaches.

---

## Integration with Pipeline

### Replace ML Scorer in Agent 2.5

**Old (ML-based):**
```python
from src.agents.agent2_5_scorer import HybridMLScorer
scorer = HybridMLScorer()
result = scorer.predict(feature_vector)
```

**New (LLM-based):**
```python
from src.agents.agent2_5_llm_scorer import LLMScorer
scorer = LLMScorer(api_key=API_KEY)
result = scorer.score_match(profile_data, job_data)
```

### Update Main Pipeline

1. **Load API key from environment:**
   ```python
   import os
   from dotenv import load_dotenv
   
   load_dotenv()
   API_KEY = os.getenv("OPENROUTER_API_KEY")
   ```

2. **Initialize LLM scorer:**
   ```python
   from src.agents.agent2_5_llm_scorer import LLMScorer
   scorer = LLMScorer(api_key=API_KEY)
   ```

3. **Use in Agent 3 (Ranking):**
   ```python
   # Agent 1: Parse resume
   parsed_profile = agent1.parse_resume(resume_text)
   
   # Agent 1: Parse jobs
   parsed_jobs = [agent1.parse_job(job) for job in jobs_data]
   
   # Agent 2.5: Score matches (LLM)
   top_matches = scorer.batch_score(parsed_profile, parsed_jobs, top_k=10)
   
   # Agent 3: Rank and present
   ranked_results = agent3.rank_matches(top_matches)
   ```

---

## Cost Estimation

### OpenRouter GPT-OSS-20B Pricing

- **Input tokens**: ~$0.00015 per 1K tokens
- **Output tokens**: ~$0.0006 per 1K tokens
- **Average per match**: ~1500 total tokens = **~$0.0008**

### Monthly Cost Examples

| Scenario | Matches/Month | Cost/Month |
|----------|--------------|------------|
| Small college project | 100 | $0.08 |
| Medium testing | 1,000 | $0.80 |
| Large deployment | 10,000 | $8.00 |
| Enterprise | 100,000 | $80.00 |

üí° **Very affordable for your college project!**

---

## Response Format

LLM scorer returns structured JSON:

```json
{
  "match_label": "High" | "Medium" | "Low",
  "confidence": 0.85,
  "reasoning": "Strong technical match with 80% skill overlap...",
  "skill_match_score": 0.80,
  "experience_match_score": 0.90,
  "key_strengths": ["Python", "Django", "PostgreSQL"],
  "key_gaps": ["Redis", "Docker"],
  "model_used": "openai/gpt-oss-20b",
  "tokens_used": {
    "prompt": 450,
    "completion": 180,
    "total": 630
  }
}
```

---

## Fallback Behavior

If API fails (network error, quota exceeded), scorer automatically falls back to **rule-based scoring**:

```python
# Automatic fallback - no changes needed
result = scorer.score_match(profile, job)

# Check if fallback was used
if result.get("model_used") == "fallback_rules":
    print("‚ö†Ô∏è Used fallback scoring (API unavailable)")
```

---

## Environment Variables (Optional)

Create `.env` file in project root:

```env
# OpenRouter API Key
OPENROUTER_API_KEY=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

# Optional: Alternative model
OPENROUTER_MODEL=openai/gpt-oss-20b
```

Load in code:
```python
from dotenv import load_dotenv
import os

load_dotenv()
API_KEY = os.getenv("OPENROUTER_API_KEY")
```

---

## Troubleshooting

### Error: "Invalid API key"
- Verify key starts with `sk-or-v1-`
- Check OpenRouter dashboard for key status
- Ensure no extra spaces in key

### Error: "Rate limit exceeded"
- Wait 60 seconds and retry
- Check your OpenRouter quota
- Consider adding retry logic with exponential backoff

### Error: "Model not available"
- Model name: `openai/gpt-oss-20b`
- Check OpenRouter models page for availability
- Try alternative: `openai/gpt-4o-mini`

### Slow response times
- Normal: 2-5 seconds per match
- Batch scoring: Use `batch_score()` for efficiency
- Consider caching results for repeated matches

---

## Next Steps

1. ‚úÖ **Test the scorer:**
   ```powershell
   python tests/test_agent2_5_llm_scorer.py
   ```

2. ‚úÖ **Compare with ML:**
   ```powershell
   python compare_scorers.py
   ```

3. ‚úÖ **Integrate into pipeline:**
   - Update `src/api.py` to use LLM scorer
   - Update `streamlit_app/app.py` to display reasoning

4. ‚úÖ **Deploy:**
   - Add `.env` file with API key
   - Update FastAPI endpoints
   - Test end-to-end flow

---

## Support

**Documentation:**
- Agent 2.5 LLM: `src/agents/agent2_5_llm_scorer.py`
- Tests: `tests/test_agent2_5_llm_scorer.py`
- Comparison: `compare_scorers.py`

**OpenRouter:**
- Dashboard: https://openrouter.ai/
- Docs: https://openrouter.ai/docs
- Models: https://openrouter.ai/models

---

**Created:** December 11, 2025  
**Model:** OpenAI GPT-OSS-20B via OpenRouter  
**Status:** ‚úÖ Ready for Production
