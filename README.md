# üéØ AI-Powered Resume-Job Matching System

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.29+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-Academic-yellow.svg)](LICENSE)

An intelligent multi-agent system for automated resume-job matching using machine learning, natural language processing, and LLM-powered explanations. Built for academic research and real-world HR automation.

---

## üìã Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Technology Stack](#-technology-stack)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Module Documentation](#-module-documentation)
- [API Reference](#-api-reference)
- [Testing](#-testing)
- [Deployment](#-deployment)
- [Performance Metrics](#-performance-metrics)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## üåü Overview

This system automates the resume screening process using a sophisticated multi-agent architecture that combines:

- **Machine Learning**: Random Forest classifier trained on 35,730+ labeled resume-job pairs
- **Natural Language Processing**: spaCy and NLTK for intelligent text parsing
- **LLM Integration**: OpenAI GPT and local Ollama models for contextual explanations
- **ATS Engine**: Applicant Tracking System with advanced scoring algorithms
- **Real-time Analytics**: Comprehensive matching history and performance tracking

### Project Highlights

| Metric | Value |
|--------|-------|
| **Training Dataset** | 35,730 labeled pairs |
| **Model Accuracy** | 96% on validation set |
| **Job Templates** | 500+ Egyptian tech jobs |
| **Processing Speed** | < 2 seconds per CV |
| **Supported Formats** | PDF, DOCX, TXT |
| **Languages** | English, Arabic (experimental) |

---

## ‚ú® Key Features

### ü§ñ Multi-Agent Architecture
- **Agent 1**: Profile & Job Parser (spaCy + NLTK)
- **Agent 2**: Feature Engineering & Extraction
- **Agent 3**: ML-Powered Scoring & Ranking
- **Agent 4**: LLM Explanations & Insights

### üéØ Core Capabilities
- ‚úÖ Automated CV parsing and skill extraction
- ‚úÖ Intelligent job-candidate matching
- ‚úÖ ATS-compatible scoring system
- ‚úÖ AI-generated match explanations
- ‚úÖ Batch processing support
- ‚úÖ Real-time analytics dashboard
- ‚úÖ RESTful API for integration
- ‚úÖ Interactive web interface

### üîí Enterprise Features
- Environment-based configuration
- Comprehensive logging and monitoring
- Match history tracking
- Customizable scoring thresholds
- Multi-format document support

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER INTERFACE                          ‚îÇ
‚îÇ                    (Streamlit Web App)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FASTAPI GATEWAY                            ‚îÇ
‚îÇ                   (REST API Endpoints)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND ORCHESTRATOR                         ‚îÇ
‚îÇ              (Coordinates Multi-Agent System)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº               ‚ñº               ‚ñº                ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇAgent 1 ‚îÇ     ‚îÇAgent 2 ‚îÇ     ‚îÇAgent 3 ‚îÇ      ‚îÇAgent 4 ‚îÇ
    ‚îÇ Parser ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇFeatures‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Scorer ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇExplain ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ               ‚îÇ               ‚îÇ                ‚îÇ
         ‚ñº               ‚ñº               ‚ñº                ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ATS ENGINE                           ‚îÇ
    ‚îÇ         (Advanced Scoring & Ranking Logic)             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              DATA PERSISTENCE LAYER                    ‚îÇ
    ‚îÇ   (Match History, Logs, Processed Profiles, Reports)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow Pipeline

```
CV Upload (PDF/DOCX/TXT)
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Agent 1: Parser   ‚îÇ  ‚ûú Extract skills, experience, education
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Agent 2: Features   ‚îÇ  ‚ûú Generate feature vectors, compute overlaps
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent 3: Scorer    ‚îÇ  ‚ûú ML classification + ATS scoring
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Agent 4: Explainer  ‚îÇ  ‚ûú Generate AI explanations (Ollama/GPT)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
Final Results (JSON)
    ‚îú‚îÄ Match scores
    ‚îú‚îÄ Rankings
    ‚îú‚îÄ Explanations
    ‚îî‚îÄ Recommendations
```

---

## üõ†Ô∏è Technology Stack

### Core Technologies

| Category | Technologies |
|----------|-------------|
| **Language** | Python 3.9+ |
| **Web Framework** | FastAPI 0.104, Uvicorn |
| **UI Framework** | Streamlit 1.29 |
| **ML/AI** | scikit-learn, spaCy 3.7, NLTK 3.8 |
| **LLM Integration** | OpenAI API, Ollama, LangChain 0.3 |
| **Data Processing** | Pandas 2.1, NumPy 1.26 |
| **Document Parsing** | PyMuPDF, python-docx, pdfminer.six |
| **Orchestration** | CrewAI 0.86 |
| **Testing** | pytest, pytest-cov |

### Key Libraries

```python
# NLP & ML
spacy==3.7.2              # Named entity recognition, POS tagging
nltk==3.8.1               # Text processing, tokenization
scikit-learn              # Random Forest classifier

# LLM Integration
openai==1.51.0            # GPT models via OpenRouter
ollama==0.4.4             # Local LLM (Llama 3.2)
langchain==0.3.13         # LLM orchestration
crewai==0.86.0            # Multi-agent coordination

# Web & API
fastapi==0.104.1          # REST API framework
streamlit==1.29.0         # Interactive UI
uvicorn==0.24.0           # ASGI server

# Document Processing
PyMuPDF==1.23.8           # PDF parsing
python-docx==1.1.0        # DOCX parsing
pdfminer.six==20221105    # Advanced PDF extraction
```

---

## üìÅ Project Structure

```
HR-Project/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ agents/                      # Multi-agent system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent1_parser.py         # CV/Job parser (spaCy + NLTK)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent2.py                # Feature engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent3.py                # ML scoring & ranking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent4_explainer.py      # LLM explanations (Ollama/GPT)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                       # Utility modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_processing.py       # NLP utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ skill_extraction.py      # Skill matching logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_parser.py           # Document parsers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py                # Configuration loader
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api.py                       # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ backend.py                   # Backend orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ ats_engine.py                # ATS scoring engine
‚îÇ   ‚îî‚îÄ‚îÄ match_history.py             # Match tracking system
‚îÇ
‚îú‚îÄ‚îÄ üìÅ streamlit_app/                # Web interface
‚îÇ   ‚îú‚îÄ‚îÄ app.py                       # Main Streamlit app
‚îÇ   ‚îú‚îÄ‚îÄ tabs/                        # UI tabs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload_tab.py            # CV upload interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ results_tab.py           # Match results display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics_tab.py         # Analytics dashboard
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interpretation_tab.py    # AI explanations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ components/                  # Reusable UI components
‚îÇ   ‚îú‚îÄ‚îÄ theme.py                     # UI styling
‚îÇ   ‚îî‚îÄ‚îÄ state_manager.py             # Session state management
‚îÇ
‚îú‚îÄ‚îÄ üìÅ ML/                           # Machine learning pipeline
‚îÇ   ‚îú‚îÄ‚îÄ src/                         # ML source code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py           # Dataset loading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py   # Feature extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py         # Model training
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py             # Model evaluation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                      # Trained models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.pkl           # Random Forest classifier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tfidf_vectorizer.pkl     # TF-IDF vectorizer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl        # Label encoder
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ data/                        # ML datasets
‚îÇ       ‚îî‚îÄ‚îÄ final_training_dataset_v2.csv
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                         # Application data
‚îÇ   ‚îú‚îÄ‚îÄ raw/                         # Raw datasets
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AI_Resume_Screening.csv  # Original dataset
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ json/                        # Processed data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jobs.json                # 500+ job templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsed_profiles/         # Parsed CV outputs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/                # Feature vectors
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results/                 # Match results
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ match_history.json           # Historical matches
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_cvs.json           # Test CVs
‚îÇ   ‚îî‚îÄ‚îÄ reports/                     # Generated reports
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                      # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ prepare_jobs_json.py         # Extract job templates
‚îÇ   ‚îú‚îÄ‚îÄ normalize_jobs.py            # Clean job data
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_cvs.py             # Benchmark testing
‚îÇ   ‚îî‚îÄ‚îÄ check_ports.py               # Port availability checker
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tests/                        # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_agent1_parser.py        # Parser tests
‚îÇ   ‚îú‚îÄ‚îÄ test_agent2_extraction.py    # Feature tests
‚îÇ   ‚îú‚îÄ‚îÄ test_agent2_5_llm_scorer.py  # Scorer tests
‚îÇ   ‚îú‚îÄ‚îÄ test_core.py                 # Core functionality tests
‚îÇ   ‚îî‚îÄ‚îÄ test_integration.py          # Integration tests
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/                         # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ API_KEY_SETUP.md             # API configuration guide
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_SUMMARY.md           # Project overview
‚îÇ   ‚îú‚îÄ‚îÄ DATASET_DOCUMENTATION.md     # Dataset details
‚îÇ   ‚îî‚îÄ‚îÄ final_project_structure.md   # Architecture docs
‚îÇ
‚îú‚îÄ‚îÄ üìÅ config/                       # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ rules.yaml                   # Business rules
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                       # Trained models (root)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îú‚îÄ‚îÄ .env.example                     # Environment template
‚îú‚îÄ‚îÄ .gitignore                       # Git ignore rules
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ README.md                        # This file
‚îú‚îÄ‚îÄ RUN_TEST.bat                     # Windows test runner
‚îú‚îÄ‚îÄ SETUP_LLM_SCORER.bat             # LLM setup script
‚îî‚îÄ‚îÄ START_OLLAMA.bat                 # Ollama startup script
```

---

## üì¶ Installation

### Prerequisites

- **Python**: 3.9 or higher
- **pip**: Latest version
- **Git**: For cloning the repository
- **Ollama** (Optional): For local LLM explanations

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/HR-Project.git
cd HR-Project
```

### Step 2: Install Dependencies

```bash
# Install all required packages
pip install -r requirements.txt

# Download spaCy language model
python -m spacy download en_core_web_sm

# Download NLTK data (if needed)
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### Step 3: Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys (optional for LLM features)
# Windows: notepad .env
# Linux/Mac: nano .env
```

**Environment Variables:**

```bash
# LLM API Configuration (Optional - for Agent 4)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
OPENAI_BASE_URL=https://openrouter.ai/api/v1

# Ollama Configuration (Optional - for local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# Application Settings
ENVIRONMENT=development
DEBUG=true

# Model Thresholds
MODEL_CONFIDENCE_THRESHOLD=0.60
HIGH_MATCH_THRESHOLD=0.85
MEDIUM_MATCH_THRESHOLD=0.60
```

### Step 4: Prepare Data

```bash
# Extract job templates from dataset
python scripts/prepare_jobs_json.py

# Normalize job data (optional)
python scripts/normalize_jobs.py
```

### Step 5: Train ML Models (Optional)

```bash
# Train the Random Forest classifier
cd ML
python src/model_trainer.py
cd ..
```

**Note:** Pre-trained models are included in `ML/models/` directory.

---

## üöÄ Quick Start

### Option 1: Run Full Stack (Recommended)

```bash
# Terminal 1: Start FastAPI Backend
uvicorn src.api:app --reload --port 8000

# Terminal 2: Start Streamlit UI
streamlit run streamlit_app/app.py
```

Then open your browser:
- **Streamlit UI**: http://localhost:8501
- **FastAPI Docs**: http://localhost:8000/docs
- **API Health**: http://localhost:8000/health

### Option 2: API Only

```bash
# Start FastAPI server
uvicorn src.api:app --reload --port 8000

# Test with curl
curl -X POST http://localhost:8000/match \
  -H "Content-Type: application/json" \
  -d '{"profile_text": "5 years Python developer with ML experience..."}'
```

### Option 3: Streamlit Only

```bash
# Run standalone Streamlit app
streamlit run streamlit_app/app.py
```
---

## üìö Module Documentation

### Agent 1: Profile & Job Parser

**File**: `src/agents/agent1_parser.py`

**Purpose**: Extracts structured information from raw CV text and job descriptions.

**Technologies**:
- spaCy 3.7 (primary NLP engine)
- NLTK 3.8 (fallback tokenizer)
- Custom regex patterns

**Key Functions**:

```python
def parse_profile(cv_text: str) -> dict:
    """
    Parse CV text into structured JSON.
    
    Returns:
    {
        "name": str,
        "email": str,
        "phone": str,
        "skills": List[str],
        "experience": List[dict],
        "education": List[dict],
        "years_of_experience": int
    }
    """
```

**Features**:
- Email/phone extraction via regex
- Skill identification (500+ tech skills)
- Experience timeline parsing
- Education degree recognition
- Multi-format support (PDF, DOCX, TXT)

---

### Agent 2: Feature Engineering

**File**: `src/agents/agent2.py`

**Purpose**: Generates feature vectors for ML classification.

**Key Features**:

```python
def extract_features(profile: dict, job: dict) -> dict:
    """
    Generate feature vectors from profile-job pair.
    
    Features:
    - skill_overlap_ratio: float (0-1)
    - experience_match: float (0-1)
    - education_match: float (0-1)
    - years_experience_diff: int
    - required_skills_met: int
    - preferred_skills_met: int
    - total_skills_count: int
    """
```

**Feature Categories**:
1. **Skill Matching**: Jaccard similarity, overlap ratio
2. **Experience**: Years match, seniority level
3. **Education**: Degree level, field alignment
4. **Text Similarity**: TF-IDF cosine similarity

---

### Agent 3: ML Scorer & Ranker

**File**: `src/agents/agent3.py`

**Purpose**: Classifies matches and generates scores using ML + ATS engine.

**ML Model**:
- **Algorithm**: Random Forest Classifier
- **Classes**: High, Medium, Low
- **Features**: TF-IDF (1000 features) + engineered features
- **Accuracy**: ~79% on validation set

**Scoring Logic**:

```python
def score_match(features: dict) -> dict:
    """
    Score profile-job match.
    
    Returns:
    {
        "ml_prediction": str,  # High/Medium/Low
        "ml_confidence": float,  # 0-1
        "ats_score": float,  # 0-100
        "final_score": float,  # Weighted combination
        "ranking": int
    }
    """
```

**ATS Engine Components**:
- Keyword matching (40% weight)
- Skill overlap (30% weight)
- Experience match (20% weight)
- Education match (10% weight)

---

### Agent 4: LLM Explainer

**File**: `src/agents/agent4_explainer.py`

**Purpose**: Generates human-readable explanations using LLMs.

**Supported Models**:
- **OpenAI GPT-4** (via OpenRouter)
- **Ollama Llama 3.2** (local)

**Key Functions**:

```python
def generate_explanation(
    profile: dict,
    job: dict,
    score: dict
) -> dict:
    """
    Generate AI explanation for match.
    
    Returns:
    {
        "strengths": List[str],
        "weaknesses": List[str],
        "recommendations": List[str],
        "interview_focus": List[str],
        "overall_assessment": str
    }
    """
```

**Explanation Types**:
1. **Strengths**: Why candidate is a good fit
2. **Weaknesses**: Areas of concern
3. **Recommendations**: Improvement suggestions
4. **Interview Focus**: Key topics to discuss

---

### ATS Engine

**File**: `src/ats_engine.py`

**Purpose**: Advanced scoring algorithm mimicking real ATS systems.

**Scoring Components**:

```python
class ATSEngine:
    def calculate_score(self, profile, job):
        """
        ATS scoring algorithm.
        
        Components:
        1. Keyword Match (40%)
        2. Skill Match (30%)
        3. Experience Match (20%)
        4. Education Match (10%)
        
        Returns: 0-100 score
        """
```

**Features**:
- Fuzzy keyword matching
- Synonym recognition
- Seniority level detection
- Industry-specific scoring

---

### Backend Orchestrator

**File**: `src/backend.py`

**Purpose**: Coordinates multi-agent workflow.

**Workflow**:

```python
def process_cv(cv_text: str, job_ids: List[str]) -> dict:
    """
    Full CV processing pipeline.
    
    Steps:
    1. Parse CV (Agent 1)
    2. Extract features (Agent 2)
    3. Score matches (Agent 3 + ATS)
    4. Generate explanations (Agent 4)
    5. Rank results
    6. Save to match history
    
    Returns: Complete match results
    """
```

---

### Match History System

**File**: `src/match_history.py`

**Purpose**: Tracks and persists all matching operations.

**Features**:
- JSON-based storage
- Query by profile/job/date
- Analytics aggregation
- Export to CSV/Excel

**Data Structure**:

```json
{
  "match_id": "uuid",
  "timestamp": "ISO-8601",
  "profile_id": "string",
  "job_id": "string",
  "scores": {
    "ml_score": 0.85,
    "ats_score": 78.5,
    "final_score": 81.75
  },
  "explanation": {...},
  "metadata": {...}
}
```

---

## üåê API Reference

### Base URL

```
http://localhost:8000
```

### Endpoints

#### 1. Health Check

```http
GET /health
```

**Response**:
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "timestamp": "2025-12-24T23:55:00Z"
}
```

#### 2. Match Profile

```http
POST /match
```

**Request Body**:
```json
{
  "profile_text": "5 years Python developer...",
  "job_ids": ["JOB_001", "JOB_002"],
  "top_k": 10,
  "include_explanations": true
}
```

**Response**:
```json
{
  "profile_id": "PROF_12345",
  "matches": [
    {
      "job_id": "JOB_001",
      "job_title": "Senior Python Developer",
      "scores": {
        "ml_score": 0.85,
        "ats_score": 78.5,
        "final_score": 81.75
      },
      "explanation": {
        "strengths": ["Strong Python experience", "..."],
        "weaknesses": ["Limited cloud experience"],
        "recommendations": ["..."]
      }
    }
  ],
  "processing_time_ms": 1850
}
```

#### 3. Get Jobs

```http
GET /jobs?category=software&seniority=senior
```

**Response**:
```json
{
  "total": 500,
  "jobs": [
    {
      "job_id": "JOB_001",
      "title": "Senior Python Developer",
      "category": "Software Engineering",
      "required_skills": ["Python", "Django", "PostgreSQL"],
      "experience_years": 5
    }
  ]
}
```

#### 4. Get Match History

```http
GET /history?profile_id=PROF_12345&limit=50
```

**Response**:
```json
{
  "total_matches": 150,
  "matches": [...]
}
```

---

## üß™ Testing

### Run All Tests

```bash
# Run all tests with verbose output
pytest tests/ -v

# Run with coverage report
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_agent1_parser.py -v

# Run with markers
pytest tests/ -m "not slow" -v
```

### Test Structure

```
tests/
‚îú‚îÄ‚îÄ test_agent1_parser.py          # Parser unit tests
‚îú‚îÄ‚îÄ test_agent2_extraction.py      # Feature extraction tests
‚îú‚îÄ‚îÄ test_core.py                   # Core functionality tests
‚îú‚îÄ‚îÄ test_integration.py            # End-to-end tests
‚îú‚îÄ‚îÄ test_matching.py               # Matching algorithm tests
‚îî‚îÄ‚îÄ test_skill_logic.py            # Skill extraction tests
```

### Coverage Report

```bash
# Generate HTML coverage report
pytest tests/ --cov=src --cov-report=html

# Open report
# Windows: start htmlcov/index.html
# Linux/Mac: open htmlcov/index.html
```

---

## üöÄ Deployment

### Streamlit Cloud (Recommended)

1. **Push to GitHub**:
```bash
git add .
git commit -m "Ready for deployment"
git push origin main
```

2. **Deploy on Streamlit Cloud**:
   - Go to [share.streamlit.io](https://share.streamlit.io)
   - Connect your GitHub repository
   - Set main file: `streamlit_app/app.py`
   - Add secrets in dashboard (API keys)

3. **Configure Secrets**:
```toml
# .streamlit/secrets.toml
[api]
OPENAI_API_KEY = "your-key"
OLLAMA_BASE_URL = "http://your-ollama-server:11434"

[model]
CONFIDENCE_THRESHOLD = 0.60
```

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
RUN python -m spacy download en_core_web_sm

COPY . .

EXPOSE 8000 8501

CMD ["sh", "-c", "uvicorn src.api:app --host 0.0.0.0 --port 8000 & streamlit run streamlit_app/app.py --server.port 8501"]
```

```bash
# Build and run
docker build -t hr-matching .
docker run -p 8000:8000 -p 8501:8501 hr-matching
```

---

## üìä Performance Metrics

### ML Model Performance

| Metric | Value |
|--------|-------|
| **Accuracy** | 96.3% |
| **Precision (High)** | 85% |
| **Recall (High)** | 83% |
| **F1-Score (High)** | 84% |
| **Training Samples** | 25,011 |
| **Validation Samples** | 5,359 |
| **Test Samples** | 5,360 |

### System Performance

| Operation | Average Time |
|-----------|--------------|
| **CV Parsing** | 0.3s |
| **Feature Extraction** | 0.2s |
| **ML Prediction** | 0.1s |
| **LLM Explanation** | 1.2s |
| **Total Pipeline** | < 2s |

---

## üêõ Troubleshooting

### Common Issues

#### 1. spaCy Model Not Found

```bash
# Solution
python -m spacy download en_core_web_sm
```

#### 2. Port Already in Use

```bash
# Change ports
uvicorn src.api:app --port 8001
streamlit run streamlit_app/app.py --server.port 8502
```

#### 3. Ollama Connection Error

```bash
# Check Ollama status
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve

# Pull model
ollama pull llama3.2:3b
```

#### 4. Missing Environment Variables

```bash
# Verify .env file exists
cat .env

# Check variables are loaded
python -c "from dotenv import load_dotenv; load_dotenv(); import os; print(os.getenv('OPENAI_API_KEY'))"
```

---

## ü§ù Contributing

This is an academic project. For contributions:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

## üìÑ License

**Academic/Educational Use Only**

This project is developed for academic research and educational purposes.

---

## üôè Acknowledgments

- **Dataset**: AI Resume Screening Dataset (35,730+ records)
- **NLP**: spaCy, NLTK
- **ML**: scikit-learn
- **LLM**: OpenAI, Ollama
- **Frameworks**: FastAPI, Streamlit
- **Job Data**: Wuzzuf Egypt Tech Jobs

---

## üìû Support

For issues and questions:
- **GitHub Issues**: [Create an issue](https://github.com/yourusername/HR-Project/issues)
- **Documentation**: See `docs/` directory
- **API Docs**: http://localhost:8000/docs (when running)

---

**Status**: ‚úÖ Production Ready  
**Version**: 1.0.0  
**Last Updated**: December 24, 2025  
**Maintained By**: HR-Project Team
