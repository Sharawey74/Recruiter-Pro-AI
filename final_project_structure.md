# ðŸŽ¯ Final Project Structure (Updated)

**Project:** Resume-Job Matching Multi-Agent System  
**Training Data:** `final_training_dataset_v2.csv` (35,730 records)  
**Target:** College Project - Streamlit Cloud Deployment  
**Updated:** December 7, 2025  

---

## ðŸ“Š Key Updates

âœ… **Removed:** `notebooks/` folder  
âœ… **Added:** `.env` configuration for LLM API keys  
âœ… **Updated:** Agent 2.5 to classifier-only (no regression)  
âœ… **Implemented:** Rule-based probability smoothing  

---

## ðŸ—ï¸ COMPLETE PROJECT STRUCTURE

```
resume-job-matching/
â”‚
â”œâ”€â”€ ðŸ“ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ final_training_dataset_v2.csv          # Your existing dataset (35,730 records)
â”‚   â”‚
â”‚   â””â”€â”€ json/
â”‚       â”œâ”€â”€ jobs.json                               # Job templates (100 diverse jobs)
â”‚       â”œâ”€â”€ jobs_features.json                      # Precomputed job features (optional)
â”‚       â”œâ”€â”€ resumes_sample.json                     # Sample resumes for testing
â”‚       â”‚
â”‚       â”œâ”€â”€ parsed_profiles/                        # Agent 1 outputs
â”‚       â”‚   â”œâ”€â”€ tmp_001.json
â”‚       â”‚   â””â”€â”€ tmp_002.json
â”‚       â”‚
â”‚       â”œâ”€â”€ features/                               # Agent 2 outputs (optional)
â”‚       â”‚   â””â”€â”€ tmp_001_JOB_SE_BACK_001.json
â”‚       â”‚
â”‚       â”œâ”€â”€ results/                                # Agent 3 outputs
â”‚       â”‚   â””â”€â”€ tmp_001_results.json
â”‚       â”‚
â”‚       â””â”€â”€ logs/
â”‚           â””â”€â”€ predictions.log                     # Agent 4 logs
â”‚
â”œâ”€â”€ ðŸ“ models/
â”‚   â”œâ”€â”€ classifier.pkl                              # âœ… Trained classification model
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl                        # TF-IDF vectorizer
â”‚   â”œâ”€â”€ label_encoder.pkl                           # Label encoder (High/Medium/Low)
â”‚   â””â”€â”€ model_metadata.json                         # Model training info
â”‚
â”œâ”€â”€ ðŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent1_parser.py                        # âœ… Profile & Job Parser (spaCy + NLTK)
â”‚   â”‚   â”œâ”€â”€ agent2_features.py                      # Feature Generator
â”‚   â”‚   â”œâ”€â”€ agent2_5_scorer.py                      # âœ… ML Scorer (Classifier + Rules)
â”‚   â”‚   â”œâ”€â”€ agent3_ranker.py                        # Decision & Explanation
â”‚   â”‚   â””â”€â”€ agent4_analytics.py                     # Analytics & Logging
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_processing.py                      # NLP utilities
â”‚   â”‚   â”œâ”€â”€ skill_extraction.py                     # Skill extraction logic
â”‚   â”‚   â””â”€â”€ config.py                               # Configuration loader
â”‚   â”‚
â”‚   â”œâ”€â”€ models.py                                   # Model loading utilities
â”‚   â””â”€â”€ api.py                                      # FastAPI application
â”‚
â”œâ”€â”€ ðŸ“ streamlit_app/
â”‚   â”œâ”€â”€ app.py                                      # Main Streamlit app
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_upload_cv.py                          # CV upload page
â”‚   â”‚   â”œâ”€â”€ 2_match_results.py                      # Results display
â”‚   â”‚   â”œâ”€â”€ 3_analytics.py                          # Analytics dashboard
â”‚   â”‚   â””â”€â”€ 4_job_management.py                     # Job templates management
â”‚   â”‚
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cv_uploader.py                          # CV upload component
â”‚       â”œâ”€â”€ job_selector.py                         # Job selection component
â”‚       â””â”€â”€ results_display.py                      # Results visualization
â”‚
â”œâ”€â”€ ðŸ“ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_agent1_parser.py                       # Parser unit tests
â”‚   â”œâ”€â”€ test_agent2_features.py                     # Feature gen tests
â”‚   â”œâ”€â”€ test_agent2_5_scorer.py                     # âœ… ML scorer tests
â”‚   â”œâ”€â”€ test_agent3_ranker.py                       # Ranker tests
â”‚   â””â”€â”€ test_utils.py                               # Utility function tests
â”‚
â”œâ”€â”€ ðŸ“ scripts/
â”‚   â”œâ”€â”€ prepare_jobs_json.py                        # âœ… Extract jobs from dataset
â”‚   â”œâ”€â”€ train_models.py                             # âœ… Train classifier (no regression)
â”‚   â”œâ”€â”€ compute_metrics.py                          # Analyze logs
â”‚   â””â”€â”€ export_sample_data.py                       # Create sample JSONs
â”‚
â”œâ”€â”€ ðŸ“ config/
â”‚   â”œâ”€â”€ rules.yaml                                  # Business rules for smoothing
â”‚   â””â”€â”€ thresholds.yaml                             # Decision thresholds
â”‚
â”œâ”€â”€ ðŸ“„ .env.example                                 # âœ… Environment template
â”œâ”€â”€ ðŸ“„ .env                                         # âœ… Actual env vars (gitignored)
â”œâ”€â”€ ðŸ“„ .gitignore                                   # âœ… Updated with .env
â”œâ”€â”€ ðŸ“„ requirements.txt                             # âœ… Updated with python-dotenv
â”œâ”€â”€ ðŸ“„ README.md                                    # Project documentation
â””â”€â”€ ðŸ“„ run_local.sh                                 # Local startup script
```

---

## ðŸ”§ Agent 2.5: Classifier + Rule Smoothing

### How It Works

```python
# Step 1: Classifier predicts probabilities
classifier.predict_proba(features)
# Output: [High: 0.78, Medium: 0.19, Low: 0.03]

# Step 2: Apply business rules
if High_prob > 0.85:
    enforce label = "High"
elif High_prob < 0.60 and predicted_label == "High":
    downgrade to "Medium"
elif Medium_prob < 0.35 and predicted_label == "Medium":
    downgrade to "Low"

# Step 3: Use probability as score
score = prob_dict[smoothed_label]  # e.g., 0.78

# Output
{
  "predicted_label": "High",
  "predicted_score": 0.78,
  "class_probabilities": {
    "High": 0.78,
    "Medium": 0.19,
    "Low": 0.03
  },
  "confidence": 0.85,
  "smoothing_flags": ["high_confidence_enforced"]
}
```

### Smoothing Rules

```python
rules = {
    'high_confidence_threshold': 0.85,      # Enforce High if prob > 0.85
    'medium_confidence_threshold': 0.60,    # Downgrade High if prob < 0.60
    'low_confidence_floor': 0.35,           # Downgrade Medium if prob < 0.35
    'ambiguous_threshold': 0.10             # Flag if top 2 differ by < 0.10
}
```

---

## ðŸ” Environment Configuration

### `.env.example` (Template)

```bash
# LLM API Configuration (for Agent 3 - Optional)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.7

# Application Settings
ENVIRONMENT=development
DEBUG=true

# Model Settings
MODEL_CONFIDENCE_THRESHOLD=0.60
HIGH_MATCH_THRESHOLD=0.85
MEDIUM_MATCH_THRESHOLD=0.60
```

### Setup

```bash
# Copy template
cp .env.example .env

# Edit .env with your actual API keys
nano .env
```

---

## ðŸ“¦ Installation & Setup

### 1. Install Dependencies

```bash
# Install packages directly
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm
```

### 2. Prepare Data

```bash
# Extract jobs from dataset
python scripts/prepare_jobs_json.py
```

### 3. Train Model

```bash
# Train classifier (no regression)
python scripts/train_models.py
```

### 4. Test Agent 2.5

```bash
# Test scorer
python src/agents/agent2_5_scorer.py
```

### 5. Run Application

```bash
# Terminal 1: Start FastAPI
uvicorn src.api:app --reload --port 8000

# Terminal 2: Start Streamlit
streamlit run streamlit_app/app.py
```

---

## ðŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test
pytest tests/test_agent2_5_scorer.py -v
```

---

## ðŸ“Š Model Training Output

```
Loading final_training_dataset_v2.csv...
âœ“ Loaded 35,730 training examples

Match distribution:
Low       13822
Medium    11269
High      10639

Vectorizing text with TF-IDF...
Label classes: ['High' 'Low' 'Medium']

Splitting data (70/15/15)...
Train: 25,011 samples
Val:   5,359 samples
Test:  5,360 samples

Training Random Forest Classifier...

=== Validation Set Evaluation ===
Classification Report:
              precision    recall  f1-score   support

        High       0.85      0.83      0.84      1596
         Low       0.78      0.81      0.79      2073
      Medium       0.76      0.74      0.75      1690

    accuracy                           0.79      5359

Validation Accuracy: 0.7945
Average max probability: 0.8234

âœ… TRAINING COMPLETE!
```

---

## ðŸŽ¯ 7-Stage Implementation Plan

### Stage 0: Setup âœ…
- [x] Create project structure
- [x] Install dependencies
- [x] Configure environment

### Stage 1: Data & Parsers âœ…
- [x] Extract jobs.json
- [x] Implement Agent 1 (dual-mode parser)
- [x] Unit tests for parser

### Stage 2: Feature Engineering
- [ ] Implement Agent 2 (feature generator)
- [ ] Unit tests for features

### Stage 3: ML Model âœ…
- [x] Train classifier (no regression)
- [x] Implement Agent 2.5 with rule smoothing
- [x] Unit tests for scorer

### Stage 4: Ranking & Explainability
- [ ] Implement Agent 3 (decision engine)
- [ ] Add explanation generation
- [ ] Unit tests for ranker

### Stage 5: FastAPI Gateway
- [ ] Implement /match endpoint
- [ ] Implement /jobs endpoint
- [ ] Test API

### Stage 6: Streamlit UI
- [ ] CV upload page
- [ ] Match results display
- [ ] Analytics dashboard
- [ ] Deploy to Streamlit Cloud

### Stage 7: Testing & Documentation
- [ ] Complete unit tests
- [ ] Integration testing
- [ ] Final documentation
- [ ] Demo video

---

## ðŸš€ Deployment (Streamlit Cloud)

### 1. Prepare for Deployment

```bash
# Ensure all dependencies in requirements.txt
# Add secrets to Streamlit Cloud dashboard
```

### 2. Streamlit Secrets

Create `.streamlit/secrets.toml`:

```toml
[api]
OPENAI_API_KEY = "your-key-here"

[model]
CONFIDENCE_THRESHOLD = 0.60
```

### 3. Deploy

1. Push to GitHub
2. Connect to Streamlit Cloud
3. Deploy from `streamlit_app/app.py`

---

## ðŸ“ Key Files Created

âœ… **Setup Files:**
- `.env.example` - Environment template
- `.gitignore` - Updated with .env
- `requirements.txt` - Updated with python-dotenv

âœ… **Agent Implementations:**
- `src/agents/agent1_parser.py` - Dual-mode parser
- `src/agents/agent2_5_scorer.py` - Classifier + rule smoothing

âœ… **Scripts:**
- `scripts/prepare_jobs_json.py` - Data extraction
- `scripts/train_models.py` - Classifier training (no regression)

---

## ðŸŽ“ Next Steps

1. **Review** this updated structure
2. **Implement** Agent 2 (feature generator)
3. **Implement** Agent 3 (decision engine)
4. **Build** FastAPI gateway
5. **Create** Streamlit UI
6. **Test** end-to-end flow
7. **Deploy** to Streamlit Cloud

---

**Status:** âœ… **Structure Updated & Ready for Implementation**  
**Model Approach:** Classifier-only with probability-based scoring  
**Deployment Target:** Streamlit Cloud (free tier)  
